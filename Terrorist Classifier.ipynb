{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrorist Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this classifier is to predict the terrorist group responsible for an attack based on information about the incident, such as the weapons used and groups targeted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrorist attack data from 1970-2017 were collected from the Global Terrorism Database (GTD), available for download here: https://www.start.umd.edu/gtd/contact/.\n",
    "\n",
    "The Excel spreadsheet was converted to a .csv file to expedite handling with pandas, using a Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install xlsx2csv\n",
    "xlsx2csv ./globalterrorismdb_0617dist.xlsx ./globalterrorismdb_0617dist.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/richard/Documents/Fellowship.ai/')\n",
    "import pandas as pd\n",
    "df_full = pd.read_csv('GTD_0617dist/globalterrorismdb_0617dist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially choose to use a neural network to model this supervised classification problem, as it handles irrelevant/noisy features well. Our database includes 50+ predictor variables, and we have no prior information about their statistical power. Neural networks also automatically detect feature interactions, useful as many of our variables are subsets of each other (i.e. weapon subtype 1 and weapon 1). Theoretically, we can use the hidden layer outputs of our NN to guide our feature selection for other algorithms.\n",
    "\n",
    "We assume the data is not in a time series; i.e. each attack is independent from the subsequent or preceeding attack in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to first filter our data to remove incidents where the attacking group is unknown. We cannot meaningfully train or validate our model using samples with an unknown response variable. Then, we want to examine the distribution of attacks by group, to determine a threshold for inclusion in the model. Groups with just a handful of documented attacks will not provide the model with enough information to identify to recognize another instance of that group's attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 170,350 documented attacks, 92,044 have a known group responsible (54.0%)\n",
      "There are a total of 3,453 identified groups\n",
      "\n",
      "   attack count  % of groups  % of attacks\n",
      "0             1        49.0%          1.8%\n",
      "1             5        77.1%          4.9%\n",
      "2            10        84.8%          7.1%\n",
      "3            50        94.3%         15.4%\n",
      "4           100        96.6%         21.6%\n",
      "5           200        97.9%         28.8%\n",
      "6           500        99.1%         42.5%\n",
      "7          1000        99.5%         52.3%\n",
      "8          5000       100.0%         92.9%\n"
     ]
    }
   ],
   "source": [
    "df = df_full[df_full['gname'] != 'Unknown']\n",
    "print(\"Of the {:,} documented attacks, {:,} have a known group responsible ({:.1%})\".format((len(df_full)), len(df), len(df)/len(df_full)))\n",
    "\n",
    "groups = sorted(list(set(df['gname'])))\n",
    "print(\"There are a total of {:,} identified groups\\n\".format(len(groups)))\n",
    "groups = {g: 0 for g in groups}\n",
    "for entry in df['gname']:\n",
    "    groups[entry] += 1\n",
    "    \n",
    "dist = pd.DataFrame()\n",
    "dist['attack count'] = [1, 5, 10, 50, 100, 200, 500, 1000, 5000]\n",
    "g_subset = dist['attack count'].map(lambda x: {g:c for g,c in groups.items() if c <= x})\n",
    "dist['% of groups'] = g_subset.map(lambda x: len(x)) / len(groups)\n",
    "dist['% of attacks'] = g_subset.map(lambda x: sum(x.values())) / len(df)\n",
    "pd.options.display.float_format = '{:.1%}'.format\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, single offenders account for almost half of the unique groups, but represent less than 2% of all documented attacks. On the other hand, groups with more than 1000 attacks represent almost half of all documented attacks, despite accounting for only 0.5% of the total groups. This indicates that a large proportion of the attacks are carried out by a few active groups.\n",
    "\n",
    "We tentatively choose to only include groups with at least 5 documented attacks. Including more groups in the training set will allow the model to learn and recognize a larger subset of groups in the test set, but with the tradeoff of being more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we perform any preprocessing, we first split the data into training and test sets. This way, any data transformations can be first learnt from the training set and then applied to the held-out test set. We want to avoid using any knowledge gleaned from the test set to inform our processing of the training set. We use sklearn's train_test_split to randomly select 25% of samples for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dfy = df['gname']\n",
    "dfx = df.drop('gname', axis=1)\n",
    "train_x, test_x, train_y, test_y = train_test_split(dfx, dfy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can filter our training set for groups with at least 5 documented attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_thres = 5\n",
    "newgroups = []\n",
    "for group, count in groups.items():\n",
    "    if count >= group_thres:\n",
    "        newgroups.append(group)    \n",
    "train_x = train_x[train_y.isin(newgroups)]\n",
    "train_y = train_y[train_y.isin(newgroups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database contains 135 variables, but many of these were excluded for the following reasons:\n",
    "* Several variables are encoded as both text and unique integer codes (i.e. country = 4 corresponds to country_txt = Albania). We only include the integer encodings.\n",
    "* Some information is too specific to be generalized by the model, i.e. Names of Targets, Latitude/Longitude of attack.\n",
    "* Text entries (i.e. Summary, Add'l Notes) were not included as they would need to be parsed by word count.\n",
    "\n",
    "After manually filtering for relevant features, we are left with 52 predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = pd.read_csv('/Users/richard/Documents/Fellowship.ai/variables LONGER.txt')\n",
    "cols = list(cols['headers'])\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45 of the 52 selected variables are categorical. Since these variables have no inherent logical ordering, we need to binarize them using one-hot encoding. Several categorical variables contain a large number of values (i.e. 190 countries), many of which are rare and thus do not carry much statistical power. Thus, we provide a method to aggregate these rare values into an \"other\" category, to reduce the number of dummy variables. This also identifies unknown values, which can show up as a negative integer (i.e. â€“9) or as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "def aggregate(df_or, col, threshold=1):\n",
    "    df = df_or[df_or[col].notnull()]\n",
    "    set_vals = list(set(df[col]))\n",
    "    # for variables with fewer values, faster to slice dataframe\n",
    "    if (len(set_vals) < 30):\n",
    "        vals = {v: sum(df[col] == v) for v in set_vals}\n",
    "    # for variables with many values, faster to iterate through elements\n",
    "    else:  \n",
    "        vals = {v:0 for v in set_vals}\n",
    "        for index, row in df.iterrows():\n",
    "            vals[row[col]] += 1\n",
    "    \n",
    "    used, unused, unknown = {}, {}, {}\n",
    "    if len(df_or) - len(df) > 0:\n",
    "        unknown['NaN'] = len(df_or) - len(df)\n",
    "    for k,v in vals.items():\n",
    "        if isinstance(k, numbers.Real) and k < 0:\n",
    "            unknown[k] = v\n",
    "        else:\n",
    "            if v < threshold:\n",
    "                unused[k] = v\n",
    "            else:\n",
    "                used[k] = v\n",
    "    print(sum(unused.values())/len(df_or))\n",
    "    return used, unused, unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sets of common, rare, and unknown values are passed into the one-hot encoder to convert the categorical variable into a set of dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(df, col, used, unused, unknown):\n",
    "    for val in used:\n",
    "        df[col + '_' + str(val)] = (df[col] == val)\n",
    "    if unused:\n",
    "        df[col + '_other'] = df[col].isin(unused)\n",
    "    if unknown:\n",
    "        df[col + '_unknown'] = (df[col].isin(unknown) | df[col].isnull())\n",
    "    \n",
    "    # drop last column since only need k-1 dummy variables\n",
    "    df.drop([col, df.columns[-1]], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scalars = ['nkill', 'nwound', 'nhostkid', 'nhours', 'ndays', 'ransomamt', 'nperps']\n",
    "categories = [c for c in cols if c not in scalars]\n",
    "cate_thres = 100\n",
    "for col in categories[1:]:\n",
    "    used, unused, unknown = aggregate(train_x, col, cate_thres)\n",
    "    one_hot(train_x, col, used, unused, unknown)\n",
    "    one_hot(test_x, col, used, unused, unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dummy variables can be controlled by increasing the frequency threshold for a value to be aggregated. For our first pass, we will aggregate all values with fewer than 100 occurrences. This threshold is a model hyperparameter that we will optimize later.\n",
    "\n",
    "| Frequency Threshold | # Dummy Variables |\n",
    "| --- | ---|\n",
    "| 1 | 1230 |\n",
    "| 10 | 727 |\n",
    "| 100 | 408 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning scalar variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scalar variables, we need to add a binary variable to designate unknown values. Certain variables have unique flag values to indicate that the value is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_scalar(df, col, special_unknowns=None):\n",
    "    unknown_rows = (df[col].isnull()) | (df[col] < 0) | \\\n",
    "                 (col in special_unknowns and df[col] == special_unknowns[col])\n",
    "    if any(unknown_rows):        \n",
    "        df[col + '_unknown'] = unknown_rows\n",
    "        df.loc[unknown_rows, col] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_unknowns = {'nhours': 999}\n",
    "for col in scalars:\n",
    "    train_x = clean_scalar(train_x, col, special_unknowns)\n",
    "    test_x = clean_scalar(test_x, col, special_unknowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize our data since the MLP model is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = StandardScaler()\n",
    "scalers.fit(train_x)\n",
    "trainx = scalers.transform(train_x)\n",
    "testx = scalers.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the MLP model using the default number of hidden layers, and then predict the groups responsible in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(20,20))\n",
    "model.fit(trainx, train_y)\n",
    "pred = model.predict(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use sklearn's classification report to evaluate model performance over different data filtering thresholds. As mentioned above, we are currently including groups with at least 5 documented attacks, and aggregating feature values with fewer than 100 instances. Below we evaluate how model performance and efficiency change with these two hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label inclusion threshold | Feature aggregation threshold | precision | recall | f1-score | time (s) |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 5 | 1 | 0.72 | 0.73 | 0.72 | 665 | <<<<\n",
    "| 5 | 10 | 0.66 | 0.70 | 0.67 | 411 |\n",
    "| 5 | 50 | 0.67 | 0.71 | 0.68 | 477 |\n",
    "| 5 | 100 | 0.67 | 0.71 | 0.68 | 572 |\n",
    "| 5 | 500 | 0.65 | 0.70 | 0.66 | 406 |\n",
    "| 2 | 100 | 0.68 | 0.71 | 0.68 | 924 |\n",
    "| 3 | 100 | 0.68 | 0.72 | 0.69 | 658 |\n",
    "| 5 | 100 | 0.67 | 0.71 | 0.68 | 572 | default\n",
    "| 10 | 100 | 0.66 | 0.71 | 0.68 | 472 |\n",
    "| 50 | 100 | 0.58 | 0.68 | 0.62 | 114 |\n",
    "| 100 | 100 | 0.53 | 0.66 | 0.58 | 108 |\n",
    "\n",
    "As expected, increasing the threshold for aggregating rare feature improves the training speed, without significantly impacting model performance. This suggests that rare features do not have strong predictive power. We can safely use a threshold value of 100 without sacrificing performance.?????????\n",
    "\n",
    "Reducing the inclusion threshold for label frequency improves model performance up to a certain point, after which there are diminishing returns. We choose a threshold value of 5 attacks/group, which approaches the maximum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform k-fold cross-validation to assess the ability of the model to generalize to unseen test cases. We must assume the attacks are independent and identically distributed, which may not be completely true as successful attacks may launch a chain of \"copycat\" attacks.\n",
    "\n",
    "Given that some groups have significantly more attacks than others, we need to consider using stratified k-fold (instead of relying on randomization) to preserve relative class frequencies in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SuppressPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = None\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self._original_stdout\n",
    "        \n",
    "def cross_validate(group_thres=5, cate_thres=100, k=4):\n",
    "    with SuppressPrints():\n",
    "        df = import_data(variable_list = \n",
    "                         '/Users/richard/Documents/Fellowship.ai/variables LONGER.txt')\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        kfolds = KFold(n_splits=k, shuffle=True)\n",
    "        splits = kfolds.split(df)\n",
    "        metrics = []\n",
    "        for train_index, test_index in splits:\n",
    "            metric = classify(group_thres, cate_thres, train_index, test_index, df)\n",
    "            metrics.append(metric)\n",
    "        \n",
    "        scores = pd.DataFrame(columns=['precision','recall','fscore'])\n",
    "        scores.loc['average'] = [np.mean(m) for m in list(zip(*metrics))[:-1]]\n",
    "        scores.loc['std'] = [np.std(m) for m in list(zip(*metrics))[:-1]]\n",
    "    print(scores)\n",
    "    return(scores)\n",
    "\n",
    "cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low standard deviation in our evaluation metrics over 4 k-folds suggests that our model is not overfitted to one particular training set. \n",
    "\n",
    "Since we have many groups with very few attacks, we use a weighted average of evaluation metrics (instead of micro/macro) to provide the most representative picture of model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
